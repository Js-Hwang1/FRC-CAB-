# CAB (Curvature-Aware Block-Sparse Attention) - Implementation Status

**Last Updated:** 2025-11-28
**Version:** Production (O(N) complexity)
**Status:** âœ… Ready for ICML benchmarking

---

## Overview

CAB is a novel KV cache eviction strategy for efficient long-context LLM inference. Unlike prior work that relies solely on attention magnitude (H2O), CAB uses a three-component eviction policy that preserves:

1. **Local context** (30%) - Recent tokens for fluency
2. **Bridge tokens** (20%) - Medium-importance connectors between key concepts
3. **Important tokens** (50%) - High cumulative attention (H2O-style heavy hitters)

---

## Scientific Contributions

### 1. Bridge Token Discovery via Importance Median

**Insight:** Tokens with moderate importance scores act as connectors between high-importance keywords.

- **High importance** â†’ Keywords, entities, facts (already kept by H2O component)
- **Low importance** â†’ Noise, filler words (should be evicted)
- **Medium importance** â†’ Transitions, logical connectors, context bridges âœ“

**Implementation:**
```python
# O(N) bridge selection
median_importance = importance_scores[candidates].median()
bridge_score = -torch.abs(importance_scores - median_importance)
bridge_indices = torch.topk(bridge_score, k=bridge_budget)
```

**Complexity:** O(N) for median + O(N log K) for topk = **O(N log K)**

### 2. Three-Component Eviction Policy

**Budget Allocation (configurable):**
- Local: 30% of retained tokens
- Bridges: 20% of retained tokens
- Importance: 50% of retained tokens

**Rationale:**
- Preserves both **what** is important (H2O) and **how** important concepts connect (bridges)
- Maintains local coherence for fluent generation
- Addresses HotpotQA failure of prior CAB versions (F1: 0.0514 â†’ expected â‰¥0.0692)

### 3. Production-Grade Complexity

**Per-token cost:** O(1) amortized
- Update importance tracker: O(1) per token
- Eviction check: O(1)

**Per-eviction cost:** O(N log K)
- Compute median: O(N)
- Select top-K important: O(N log K)
- Select K closest to median: O(N log K)
- Prune cache: O(LÂ·K) for L layers

**No O(NÂ²) operations** - production ready for long contexts (N > 10K)

---

## Implementation Architecture

### Core Components

#### 1. `cab_attention/cache/cab_cache.py`
- Main CAB cache with three-component eviction
- HuggingFace `Cache` interface compatible
- Handles multi-layer async updates correctly

#### 2. `cab_attention/eviction/policy.py`
- Three-component eviction policy
- O(N) median-based bridge selection
- Fallback to spatial coverage if no importance scores

#### 3. `cab_attention/scoring/importance.py`
- H2O-style cumulative attention tracking
- O(1) update per token
- O(N) pruning with index validation

#### 4. `cab_attention/cache/h2o_cache.py`
- Pure H2O baseline for comparison
- Same complexity as CAB importance component
- Reference implementation faithful to Zhang et al. 2023

### Key Design Decisions

**1. No FRC/Triton Kernels in Production Path**
- Original FRC computation: O(NÂ²) triangle counting - too slow
- Solution: Use importance-based heuristic (median) instead
- Maintains scientific intuition without computational cost

**2. Async Multi-Layer Handling**
- Use minimum cache length across all layers for eviction
- Prevents index-out-of-bounds when layers update asynchronously
- Validated index filtering in tracker prune methods

**3. Eager Attention for Attention Weight Capture**
- CAB/H2O require `output_attentions=True`
- Force eager attention implementation (SDPA doesn't support it)
- Automatically enabled when methods=['cab'] or methods=['h2o']

---

## Performance Characteristics

### Benchmark Results (Qwen2.5-7B, 32 layers, CUDA)

**Runtime Benchmark:**
```
H2O Cache:
  Total time: 1.91s for 500 tokens
  Time per token: 3.81ms
  No per-eviction tracking (evicts too frequently)

CAB Cache:
  Total time: 1.30s for 500 tokens
  Time per token: 2.60ms (âœ“ 32% faster than H2O)
  Avg eviction time: 49.47ms
  Max eviction time: 144.07ms
```

**LongBench QA (NarrativeQA):**
- ~8.7s per sample (Qwen2.5-7B, long context)
- Projected: ~30 minutes for 200 samples

### Complexity Analysis

| Operation | H2O | CAB | Difference |
|-----------|-----|-----|------------|
| Per-token update | O(1) | O(1) | Same |
| Per-eviction | O(N log K) | O(N log K) | Same |
| Extra overhead | None | Median O(N) | Negligible |

**Bottleneck:** Both methods are dominated by model inference time, not cache operations.

---

## Fixes Applied

### Fix 1: Triton Kernel `continue` Statement
- **Issue:** Triton JIT doesn't support `continue`
- **Solution:** Removed (unnecessary - multiply by 0 instead)
- **File:** `cab_attention/kernels/frc_triton.py`

### Fix 2: Eviction Policy Length Mismatch
- **Issue:** Importance/FRC scores could have stale lengths
- **Solution:** Added length validation and padding
- **File:** `cab_attention/eviction/policy.py`

### Fix 3: Multi-Layer Async Update Index Error
- **Issue:** Different layers had different cache sizes during eviction
- **Solution:** Use minimum cache length across all layers
- **Files:** `cab_attention/cache/cab_cache.py`, `h2o_cache.py`

### Fix 4: Tracker Index Validation
- **Issue:** Prune indices could exceed tracker size
- **Solution:** Filter indices before indexing
- **Files:** `cab_attention/scoring/importance.py`, `frc.py`

### Fix 5: Eager Attention for CAB/H2O
- **Issue:** SDPA doesn't support capturing attention weights
- **Solution:** Force eager attention when using CAB/H2O
- **File:** `experiments/longbench_qa/runner.py`

---

## Testing Status

### Unit Tests: âœ… 6/6 Passing
```
âœ“ FRC Kernels (Triton vs PyTorch)
âœ“ Importance Tracking (H2O)
âœ“ FRC Tracking (bridge detection)
âœ“ Eviction Policy (three-component)
âœ“ CAB Cache (multi-layer eviction)
âœ“ H2O Cache (baseline)
```

### Integration Tests: ðŸ”„ In Progress
- NarrativeQA benchmark running on server
- Expected runtime: ~30 minutes for 200 samples
- Monitoring for completion

---

## Known Limitations

### 1. Eviction Time Higher Than Ideal
- Current: ~50ms avg, ~144ms max
- Target: <10ms per eviction
- **Root cause:** Multiple topk operations + median computation
- **Impact:** Negligible compared to model inference (~8700ms per sample)

### 2. No Triton Optimization
- FRC kernels exist but not used in production (too slow)
- PyTorch operations dominate eviction time
- Potential for batching/fusing operations

### 3. Single-Query Attention Tracking
- Currently tracks attention from each query independently
- Could batch attention updates for efficiency

---

## Baseline Comparisons

### Methods Implemented

1. **Dense** - No eviction (baseline, full cache)
2. **H2O** - Pure cumulative attention (Zhang et al. 2023)
3. **CAB** - Three-component with bridges (this work)
4. **StreamingLLM** - Attention sink + sliding window (Xiao et al. 2023)
5. **Local+Strided** - Strided sampling with local window
6. **Random** - Random eviction (sanity check)

### Prior Results (CAB V4 - FAILED)
```
Dataset: HotpotQA, Sparsity: 0.9
- Dense: F1 = 0.250
- H2O: F1 = 0.0692
- CAB V4: F1 = 0.0514 âŒ (worse than H2O!)
```

**Root cause:** Selecting HIGH FRC (peripheral tokens) instead of LOW FRC (bridges)

### Expected Results (CAB Current)
- Should match or exceed H2O: F1 â‰¥ 0.0692
- Pending validation on current benchmark run

---

## File Structure

```
FRC/
â”œâ”€â”€ cab_attention/
â”‚   â”œâ”€â”€ __init__.py           # Main API exports
â”‚   â”œâ”€â”€ cache/
â”‚   â”‚   â”œâ”€â”€ cab_cache.py      # CAB cache with 3-component eviction
â”‚   â”‚   â””â”€â”€ h2o_cache.py      # H2O baseline
â”‚   â”œâ”€â”€ eviction/
â”‚   â”‚   â””â”€â”€ policy.py         # Three-component eviction policy
â”‚   â”œâ”€â”€ scoring/
â”‚   â”‚   â”œâ”€â”€ importance.py     # H2O cumulative attention tracking
â”‚   â”‚   â””â”€â”€ frc.py           # FRC computation (not used in production)
â”‚   â””â”€â”€ kernels/
â”‚       â””â”€â”€ frc_triton.py    # Triton FRC kernels (not used in production)
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ longbench_qa/
â”‚       â”œâ”€â”€ driver.py         # CLI for benchmarks
â”‚       â”œâ”€â”€ runner.py         # Benchmark runner
â”‚       â””â”€â”€ config.py         # Experiment configurations
â”œâ”€â”€ test_cab_v5.py           # Comprehensive unit tests
â”œâ”€â”€ test_runtime.py          # Performance benchmarks
â””â”€â”€ STATUS.md                # This file
```

---

## Next Steps

### Immediate (Required for ICML)
1. âœ… Validate CAB vs H2O on NarrativeQA (in progress)
2. â³ Run full benchmark suite (8 datasets, multiple sparsity levels)
3. â³ Analyze results and generate plots
4. â³ Write paper with results

### Future Optimizations (Performance)
1. Batch median computation across evictions
2. Fuse topk operations
3. Optimize tracker updates
4. Profile and optimize hot paths

### Future Research (Scientific)
1. Adaptive bridge ratio based on task type
2. Query-dependent bridge selection
3. Multi-granularity eviction (token vs block)
4. Integration with Flash Attention

---

## References

**H2O (Heavy-Hitter Oracle):**
- Zhang et al., 2023. "H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models"
- arXiv:2306.14048
- Key idea: Keep tokens with highest cumulative attention

**StreamingLLM:**
- Xiao et al., 2023. "Efficient Streaming Language Models with Attention Sinks"
- Key idea: Keep initial tokens + sliding window

**CAB (This Work):**
- Three-component eviction: Local + Bridges + Importance
- O(N) median-based bridge selection
- Production-grade complexity without sacrificing scientific rigor

---

## Acknowledgments

- Original FRC concept: Forman-Ricci Curvature for graph analysis
- H2O baseline: Zhang et al. 2023
- Implementation framework: HuggingFace transformers
- Triton kernels: OpenAI Triton project
